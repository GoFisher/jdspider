# 核心代码
- ## 1. 启动爬虫
```py
    from scrapy import cmdline
    #相当于调用命令行执行
    cmdline.execute('scrapy crawl jd_book'.split())
```
- ## 2.用户实现的spider代码
```py
    import scrapy
    from jdspider import items
    from scrapy import Request
    import re
    from jdspider import pipelines

    class JdBookSpider(scrapy.Spider):
        name = 'jd_book'  # 爬虫名字
        allowed_domains = ['search.jd.com'] #爬虫的主域名
        start_urls ='https://search.jd.com/Search?keyword=%s&enc=utf-8&qrst=1&rt=1&stop=1&vt=2' #起始网站
    def start_requests(self):
        keyWold = input('请输入书籍名称:')
        pipelines.topic['path'] = keyWold
        self.start_urls = str(self.start_urls) % keyWold
        yield Request(url=self.start_urls,callback=self.parse_urls,dont_filter=True)

    # 获取每一页所有图书信息介绍节点，因为有异步加载
    def parse_urls(self,response):
        pageNum = 1 # 设置停止测试
        # total = int(re.findall('page_count:"(\d+)"',response.text)[0])
        for i in range(pageNum):
            url = '%s&page=%s'%(self.start_urls,2*i+1)
            yield Request(url,callback=self.parse)

    # 爬取每一页的信息
    def parse(self, response):
        li = response.css('ul.gl-warp.clearfix li.gl-item')
        book = items.books()
        for info in li:
            book['title'] = info.css('div.p-name').xpath('string(.//em)').extract_first()
            book['price'] = info.css('div.p-price i::text').extract_first()+"元"
            book['detail'] = info.css('i.promo-words::text').extract_first()
            if not book['detail']:
                book['detail']='无简介'
            book['author'] = info.css('span.p-bi-name a::text').extract_first()
            book['commitNum'] =info.css('div.p-commit a::text').extract_first()
            book['shopNum'] = info.css('div.p-shopnum a::text').extract_first()
            baseUrl ='//'+re.findall('="//(.*).jpg">',info.css('div.p-img img').extract_first())[0]+'.jpg'
            book['file_urls']= [response.urljoin(baseUrl)]
            yield book
```
- ## 3. 异步加载——JDMiddleware中编写selenium
```py
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from scrapy.http import HtmlResponse
    import time

    class JDMiddleware(object):
        def process_request(self,request,spider):
            if str(request.url).__contains__('page'): # 排除第一个request的请求
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--disable-gpu')
                drive = webdriver.Chrome(chrome_options=chrome_options)
                drive.maximize_window()
                drive.get(request.url)
                target = drive.find_element_by_class_name("pn-next")
                #下拉滚动条直到'pn-next'元素显示为止
                drive.execute_script("arguments[0].scrollIntoView();", target)
                time.sleep(2) #等待网页加载全部的信息，时间可以调节
                body = drive.page_source
                return HtmlResponse(drive.current_url,body=body,encoding='utf-8', request=request)
            else:
                return
```
- ## 4. 图片储存——修改文件名字
```py
    from scrapy.pipelines.files import FilesPipeline
    from os.path import basename
    # 共享变量，定义系列型数据才能共享,列表也可以
    topic = {'path':''}
    class myImagePipeline(FilesPipeline):  # 继承FilesPipeline
    def file_path(self, request, response=None, info=None): #覆写file_path，修改文件名
        return topic['path']+'/%s' % basename(request.url)
```
- ## 5.settings.py文件说明
```py

    BOT_NAME = 'jdspider'
    SPIDER_MODULES = ['jdspider.spiders']
    NEWSPIDER_MODULE = 'jdspider.spiders'
    MONGGO_DB_URI = 'mongodb://localhost:27017' #数据库地址
    DB_NAME = 'Data'  #数据库名称
    # Crawl responsibly by identifying yourself (and your website) on the user-agent
    #请求头部用户代理
    USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5'

    # 图片储存位置
    FILES_STORE = r'D:\foreign\Python\work\jdspider\jdspider\pic'

    # Obey robots.txt rules
    #爬虫协议取消
    ROBOTSTXT_OBEY = False

    #中间件开启加载的处理，还有一个SPIDER_MIDDLEWARES不要错误错误，这个是返回请求的处理
    DOWNLOADER_MIDDLEWARES = {
        'jdspider.middlewares.JDMiddleware': 300,
    }

    # 数据处理管道开启，图片和数据库两项，数字越小越早运行
    ITEM_PIPELINES = {
         'jdspider.pipelines.MongoDBPipeline': 500,
         'jdspider.pipelines.myImagePipeline':1
    }
```
